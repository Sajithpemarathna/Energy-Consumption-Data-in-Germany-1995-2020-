{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sajithpemarathna/Energy-Consumption-Data-in-Germany-1995-2020-/blob/main/Model_Implementation_Attempt_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePh9bEiHPrH-",
        "outputId": "28dec475-9c80-43ba-f955-42db0524e41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# First, install required packages\n",
        "!pip install xgboost tensorflow scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzqz4VHVQAzE",
        "outputId": "cc10a3f0-c525-4a7f-ceb3-a3450240595b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Branch Code                              Homogeneous Branches  \\\n",
            "0    CPA08-01  Products of agric., hunting and related services   \n",
            "1    CPA08-01  Products of agric., hunting and related services   \n",
            "2    CPA08-01  Products of agric., hunting and related services   \n",
            "3    CPA08-01  Products of agric., hunting and related services   \n",
            "4    CPA08-01  Products of agric., hunting and related services   \n",
            "\n",
            "                                Energy Carriers   1995   1996   1997   1998  \\\n",
            "0              Hard coal and hard coal products    888    946    829   1071   \n",
            "1  Brown coal (lignite) and brown coal products   1477   1346   1275   1056   \n",
            "2                                     Crude oil      0      0      0      0   \n",
            "3                                        Petrol   4580   3610   2976   2897   \n",
            "4                                  Diesel fuels  94156  94655  94546  94130   \n",
            "\n",
            "    1999   2000   2001  ...   2011   2012   2013   2014   2015   2016   2017  \\\n",
            "0   1185   1329   1692  ...    360   1080    248    475   1766    565    368   \n",
            "1    873    678    545  ...      0      0      0      0      0      0      0   \n",
            "2      0      0      0  ...      0      0      0      0      0      0      0   \n",
            "3   2456   1926   1573  ...    383    350    345    317    284    270    265   \n",
            "4  93709  82778  81461  ...  48796  47952  50725  52114  54741  56857  58373   \n",
            "\n",
            "    2018   2019   2020  \n",
            "0    417     34     30  \n",
            "1      0      0      0  \n",
            "2      0      0      0  \n",
            "3    261    275    241  \n",
            "4  54796  55719  56631  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 576 entries, 0 to 575\n",
            "Data columns (total 29 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   Branch Code           576 non-null    object\n",
            " 1   Homogeneous Branches  576 non-null    object\n",
            " 2   Energy Carriers       576 non-null    object\n",
            " 3   1995                  576 non-null    int64 \n",
            " 4   1996                  576 non-null    int64 \n",
            " 5   1997                  576 non-null    int64 \n",
            " 6   1998                  576 non-null    int64 \n",
            " 7   1999                  576 non-null    int64 \n",
            " 8   2000                  576 non-null    int64 \n",
            " 9   2001                  576 non-null    int64 \n",
            " 10  2002                  576 non-null    int64 \n",
            " 11  2003                  576 non-null    int64 \n",
            " 12  2004                  576 non-null    int64 \n",
            " 13  2005                  576 non-null    int64 \n",
            " 14  2006                  576 non-null    int64 \n",
            " 15  2007                  576 non-null    int64 \n",
            " 16  2008                  576 non-null    int64 \n",
            " 17  2009                  576 non-null    int64 \n",
            " 18  2010                  576 non-null    int64 \n",
            " 19  2011                  576 non-null    int64 \n",
            " 20  2012                  576 non-null    int64 \n",
            " 21  2013                  576 non-null    int64 \n",
            " 22  2014                  576 non-null    int64 \n",
            " 23  2015                  576 non-null    int64 \n",
            " 24  2016                  576 non-null    int64 \n",
            " 25  2017                  576 non-null    int64 \n",
            " 26  2018                  576 non-null    int64 \n",
            " 27  2019                  576 non-null    int64 \n",
            " 28  2020                  576 non-null    int64 \n",
            "dtypes: int64(26), object(3)\n",
            "memory usage: 130.6+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/Sajithpemarathna/Energy-consumption-prediction-data/refs/heads/main/Use%20of%20Energy%20of%20Energy%20Carriers.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Print first few rows and basic info\n",
        "print(df.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWBftTqDQbS3",
        "outputId": "70339ca9-7fc8-4a89-91ea-dfb96b7c5e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after melting: (14976, 5)\n",
            "\n",
            "First few rows:\n",
            "    Branch Code                              Homogeneous Branches  \\\n",
            "0      CPA08-01  Products of agric., hunting and related services   \n",
            "380    CPA08-46    Wts., except of motor vehicles and motorcycles   \n",
            "381    CPA08-46    Wts., except of motor vehicles and motorcycles   \n",
            "382    CPA08-46    Wts., except of motor vehicles and motorcycles   \n",
            "383    CPA08-46    Wts., except of motor vehicles and motorcycles   \n",
            "\n",
            "                          Energy Carriers  Year  Consumption  \n",
            "0        Hard coal and hard coal products  1995          888  \n",
            "380            Other mineral oil products  1995           77  \n",
            "381                                 Gases  1995        27465  \n",
            "382                    Renewable energies  1995          117  \n",
            "383  Electricity and other energy sources  1995        29387  \n"
          ]
        }
      ],
      "source": [
        "# Melt the dataframe to convert years to a single column\n",
        "df_melted = pd.melt(df,\n",
        "                    id_vars=['Branch Code', 'Homogeneous Branches', 'Energy Carriers'],\n",
        "                    var_name='Year',\n",
        "                    value_name='Consumption')\n",
        "\n",
        "# Convert Year to numeric\n",
        "df_melted['Year'] = pd.to_numeric(df_melted['Year'])\n",
        "\n",
        "# Sort by Year\n",
        "df_melted = df_melted.sort_values('Year')\n",
        "\n",
        "# Check the result\n",
        "print(\"Shape after melting:\", df_melted.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_melted.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhyKGJFwQoRw",
        "outputId": "661b5657-0d10-4698-d349-ca315359c2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 110\n",
            "\n",
            "Feature names:\n",
            "['Branch Code_CPA08-01', 'Branch Code_CPA08-02', 'Branch Code_CPA08-03', 'Branch Code_CPA08-05', 'Branch Code_CPA08-06', 'Branch Code_CPA08-07-01', 'Branch Code_CPA08-10-01', 'Branch Code_CPA08-13-01', 'Branch Code_CPA08-16', 'Branch Code_CPA08-17', 'Branch Code_CPA08-18', 'Branch Code_CPA08-19', 'Branch Code_CPA08-20', 'Branch Code_CPA08-21', 'Branch Code_CPA08-22', 'Branch Code_CPA08-23', 'Branch Code_CPA08-24-01', 'Branch Code_CPA08-25', 'Branch Code_CPA08-26', 'Branch Code_CPA08-27', 'Branch Code_CPA08-28', 'Branch Code_CPA08-29', 'Branch Code_CPA08-30', 'Branch Code_CPA08-31-01', 'Branch Code_CPA08-33', 'Branch Code_CPA08-35', 'Branch Code_CPA08-36-01', 'Branch Code_CPA08-37-01', 'Branch Code_CPA08-41-01', 'Branch Code_CPA08-43', 'Branch Code_CPA08-45-01', 'Branch Code_CPA08-46', 'Branch Code_CPA08-47', 'Branch Code_CPA08-49', 'Branch Code_CPA08-50', 'Branch Code_CPA08-51', 'Branch Code_CPA08-52', 'Branch Code_CPA08-53', 'Branch Code_CPA08-I', 'Branch Code_CPA08-J', 'Branch Code_CPA08-K', 'Branch Code_CPA08-L', 'Branch Code_CPA08-M', 'Branch Code_CPA08-N', 'Branch Code_CPA08-O', 'Branch Code_CPA08-P', 'Branch Code_CPA08-Q', 'Branch Code_CPA08-R-01', 'Homogeneous Branches_Accommodation and food services', 'Homogeneous Branches_Administrative and support services', 'Homogeneous Branches_Air transport services', 'Homogeneous Branches_Basic metals', 'Homogeneous Branches_Basic pharmaceutical prod.and pharmac.preparations', 'Homogeneous Branches_Building and civil engineering construction works', 'Homogeneous Branches_Chemicals and chemical products', 'Homogeneous Branches_Coal and lignite', 'Homogeneous Branches_Coke and refined petroleum products', 'Homogeneous Branches_Computer, electronic and optical products', 'Homogeneous Branches_Crude petroleum and natural gas', 'Homogeneous Branches_Education services', 'Homogeneous Branches_Electrical equipment', 'Homogeneous Branches_Electricity, gas, steam and air conditioning', 'Homogeneous Branches_Fabricated metal prod.,except machinery and equip.', 'Homogeneous Branches_Financial and insurance services', 'Homogeneous Branches_Fish,aquaculture prod.,support services to fishing', 'Homogeneous Branches_Food products, beverages, tobacco products', 'Homogeneous Branches_Furniture and other manufactured goods', 'Homogeneous Branches_Human health and social work services', 'Homogeneous Branches_Information and communication services', 'Homogeneous Branches_Land transp.serv. a. transport serv. via pipelines', 'Homogeneous Branches_Machinery and equipment n.e.c.', 'Homogeneous Branches_Metal ores, other mining and quarr. prod.,services', 'Homogeneous Branches_Motor vehicles, trailers and semi-trailers', 'Homogeneous Branches_Natural water, water treatment and supply servcies', 'Homogeneous Branches_Other non-metallic mineral products', 'Homogeneous Branches_Other services', 'Homogeneous Branches_Other transport equipment', 'Homogeneous Branches_Paper and paper products', 'Homogeneous Branches_Postal and courier services', 'Homogeneous Branches_Printing and recording services', 'Homogeneous Branches_Products of agric., hunting and related services', 'Homogeneous Branches_Products of forestry, logging and related services', 'Homogeneous Branches_Professional, scientific and technical services', 'Homogeneous Branches_Public adm.,defence serv.,compuls.social sec.serv.', 'Homogeneous Branches_Real estate services', 'Homogeneous Branches_Repair, install. services of machinery and equip.', 'Homogeneous Branches_Rts., except of motor vehicles and motorcycles', 'Homogeneous Branches_Rubber and plastics products', 'Homogeneous Branches_Sewage, waste disposal, material recovery services', 'Homogeneous Branches_Specialised construction works', 'Homogeneous Branches_Textiles,wearing apparel,leather and related prod.', 'Homogeneous Branches_Warehousing a. support services for transportation', 'Homogeneous Branches_Water transport services', 'Homogeneous Branches_Wholesale,ret.trade,repair of motor veh.,motorcyc.', 'Homogeneous Branches_Wood,cork,exc.furn.,art.of straw and plaiting mat.', 'Homogeneous Branches_Wts., except of motor vehicles and motorcycles', 'Energy Carriers_Aviation turbine fuel', 'Energy Carriers_Brown coal (lignite) and brown coal products', 'Energy Carriers_Crude oil', 'Energy Carriers_Diesel fuels', 'Energy Carriers_Electricity and other energy sources', 'Energy Carriers_Gases', 'Energy Carriers_Hard coal and hard coal products', 'Energy Carriers_Heating oil', 'Energy Carriers_Heavy fuel oil', 'Energy Carriers_Other mineral oil products', 'Energy Carriers_Petrol', 'Energy Carriers_Renewable energies', 'Year_sin', 'Year_cos']\n"
          ]
        }
      ],
      "source": [
        "# Create dummy variables for categorical columns\n",
        "df_encoded = pd.get_dummies(df_melted,\n",
        "                           columns=['Branch Code', 'Homogeneous Branches', 'Energy Carriers'])\n",
        "\n",
        "# Add time-based features (optional but can help with seasonality)\n",
        "df_encoded['Year_sin'] = np.sin(2 * np.pi * df_encoded['Year']/df_encoded['Year'].max())\n",
        "df_encoded['Year_cos'] = np.cos(2 * np.pi * df_encoded['Year']/df_encoded['Year'].max())\n",
        "\n",
        "# Split features and target\n",
        "X = df_encoded.drop(['Consumption', 'Year'], axis=1)\n",
        "y = df_encoded['Consumption']\n",
        "\n",
        "# Print feature information\n",
        "print(\"Number of features:\", X.shape[1])\n",
        "print(\"\\nFeature names:\")\n",
        "print(X.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7OVsNHaQ0un",
        "outputId": "2a469e70-2d39-4690-800e-353ca5baa76e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 15 Most Important Features:\n",
            "                                               feature  importance\n",
            "98                           Energy Carriers_Crude oil    0.579816\n",
            "100  Energy Carriers_Electricity and other energy s...    0.054729\n",
            "61   Homogeneous Branches_Electricity, gas, steam a...    0.048722\n",
            "56   Homogeneous Branches_Coke and refined petroleu...    0.046125\n",
            "11                                Branch Code_CPA08-19    0.046079\n",
            "25                                Branch Code_CPA08-35    0.045571\n",
            "97   Energy Carriers_Brown coal (lignite) and brown...    0.043582\n",
            "102   Energy Carriers_Hard coal and hard coal products    0.041466\n",
            "101                              Energy Carriers_Gases    0.020032\n",
            "105         Energy Carriers_Other mineral oil products    0.017599\n",
            "107                 Energy Carriers_Renewable energies    0.015101\n",
            "108                                           Year_sin    0.012025\n",
            "109                                           Year_cos    0.011342\n",
            "96               Energy Carriers_Aviation turbine fuel    0.004338\n",
            "12                                Branch Code_CPA08-20    0.002557\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize Random Forest for feature selection\n",
        "rf_selector = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_selector.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_selector.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Print top 15 most important features\n",
        "print(\"Top 15 Most Important Features:\")\n",
        "print(feature_importance.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF7QL57mRAHB",
        "outputId": "b1475954-f989-46f7-c8c6-4049b95fd19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training set shape: (11980, 110)\n",
            "Testing set shape: (2996, 110)\n"
          ]
        }
      ],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print shapes\n",
        "print(\"\\nTraining set shape:\", X_train_scaled.shape)\n",
        "print(\"Testing set shape:\", X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Za9Qs9RSuq",
        "outputId": "dcc26ed5-b125-4efe-97a7-5bb2715dda65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Metrics:\n",
            "MSE: 375108288.00\n",
            "RMSE: 19367.71\n",
            "MAPE: 283448957423792947200.00%\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# Create and train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=7,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "xgb_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
        "xgb_rmse = np.sqrt(xgb_mse)\n",
        "xgb_mape = mean_absolute_percentage_error(y_test, xgb_pred) * 100\n",
        "\n",
        "print(\"XGBoost Metrics:\")\n",
        "print(f\"MSE: {xgb_mse:.2f}\")\n",
        "print(f\"RMSE: {xgb_rmse:.2f}\")\n",
        "print(f\"MAPE: {xgb_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "kSypP56vRZsd",
        "outputId": "1d3c1868-d9c3-41fc-c3bf-639ab8bbfd76"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "6",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 6",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-75ea69db411a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_test_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-75ea69db411a>\u001b[0m in \u001b[0;36mcreate_sequences\u001b[0;34m(X, y, sequence_length)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mX_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 6"
          ]
        }
      ],
      "source": [
        "# Create sequences for LSTM/Bi-GRU\n",
        "def create_sequences(X, y, sequence_length=5):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_seq.append(X[i:(i + sequence_length)])\n",
        "        y_seq.append(y[i + sequence_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 5\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, sequence_length)\n",
        "\n",
        "print(\"Sequence shapes:\")\n",
        "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
        "print(\"X_test_seq shape:\", X_test_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hbRcmXCARvGK"
      },
      "outputs": [],
      "source": [
        "# First, let's reset our preprocessing steps\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Ensure data is properly sorted by year\n",
        "df_encoded = df_encoded.sort_values('Year')\n",
        "\n",
        "# 2. Scale the features and target separately\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "# 3. Prepare the data differently for neural networks\n",
        "def prepare_time_series_data(X, y, sequence_length=5):\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "    X_sequences = []\n",
        "    y_sequences = []\n",
        "\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
        "        y_sequences.append(y_scaled[i + sequence_length])\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "# 4. Split data first, then create sequences\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    shuffle=False  # Important: don't shuffle for time series\n",
        ")\n",
        "\n",
        "# 5. Create sequences for training and test sets\n",
        "sequence_length = 5\n",
        "X_train_seq, y_train_seq = prepare_time_series_data(X_train, y_train, sequence_length)\n",
        "X_test_seq, y_test_seq = prepare_time_series_data(X_test, y_test, sequence_length)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Training sequences shape:\", X_train_seq.shape)\n",
        "print(\"Training targets shape:\", y_train_seq.shape)\n",
        "print(\"Test sequences shape:\", X_test_seq.shape)\n",
        "print(\"Test targets shape:\", y_test_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a01Z3Yj0R9qu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, X_train.shape[1]), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile LSTM model\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train LSTM model\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate LSTM model\n",
        "lstm_pred = lstm_model.predict(X_test_seq)\n",
        "lstm_pred = scaler_y.inverse_transform(lstm_pred)\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Calculate metrics for LSTM\n",
        "lstm_mse = mean_squared_error(y_test_actual, lstm_pred)\n",
        "lstm_rmse = np.sqrt(lstm_mse)\n",
        "lstm_mape = mean_absolute_percentage_error(y_test_actual, lstm_pred) * 100\n",
        "\n",
        "print(\"\\nLSTM Model Metrics:\")\n",
        "print(f\"MSE: {lstm_mse:.2f}\")\n",
        "print(f\"RMSE: {lstm_rmse:.2f}\")\n",
        "print(f\"MAPE: {lstm_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4dX3-AzKTFTQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "\n",
        "# Build Bi-GRU model with adjusted architecture\n",
        "bigru_model = Sequential([\n",
        "    Bidirectional(GRU(128, return_sequences=True), input_shape=(sequence_length, X_train.shape[1])),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(GRU(64)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile with adjusted learning rate\n",
        "bigru_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train Bi-GRU model\n",
        "bigru_history = bigru_model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate Bi-GRU model\n",
        "bigru_pred = bigru_model.predict(X_test_seq)\n",
        "bigru_pred = scaler_y.inverse_transform(bigru_pred)\n",
        "\n",
        "# Calculate metrics for Bi-GRU\n",
        "bigru_mse = mean_squared_error(y_test_actual, bigru_pred)\n",
        "bigru_rmse = np.sqrt(bigru_mse)\n",
        "bigru_mape = mean_absolute_percentage_error(y_test_actual, bigru_pred) * 100\n",
        "\n",
        "print(\"\\nBi-GRU Model Metrics:\")\n",
        "print(f\"MSE: {bigru_mse:.2f}\")\n",
        "print(f\"RMSE: {bigru_rmse:.2f}\")\n",
        "print(f\"MAPE: {bigru_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3lAzwo-ZUxbY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reset our preprocessing steps\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "def prepare_time_series_data_v2(X, y, sequence_length=5):\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "    # Add a rolling mean feature\n",
        "    X_df = pd.DataFrame(X_scaled)\n",
        "    X_df['rolling_mean'] = pd.DataFrame(y).rolling(window=sequence_length).mean()\n",
        "    X_df = X_df.fillna(method='bfill')\n",
        "\n",
        "    X_sequences = []\n",
        "    y_sequences = []\n",
        "\n",
        "    for i in range(len(X_df) - sequence_length):\n",
        "        X_sequences.append(X_df.iloc[i:(i + sequence_length)].values)\n",
        "        y_sequences.append(y_scaled[i + sequence_length])\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "# Prepare sequences with new method\n",
        "X_train_seq_v2, y_train_seq_v2 = prepare_time_series_data_v2(X_train, y_train, sequence_length)\n",
        "X_test_seq_v2, y_test_seq_v2 = prepare_time_series_data_v2(X_test, y_test, sequence_length)\n",
        "\n",
        "# Build a simpler but more robust LSTM model\n",
        "lstm_model_v2 = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, X_train_seq_v2.shape[2]), return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile with huber loss for robustness against outliers\n",
        "lstm_model_v2.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Add more callbacks for better training\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history_v2 = lstm_model_v2.fit(\n",
        "    X_train_seq_v2, y_train_seq_v2,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "lstm_pred_v2 = lstm_model_v2.predict(X_test_seq_v2)\n",
        "lstm_pred_v2 = scaler_y.inverse_transform(lstm_pred_v2)\n",
        "y_test_actual_v2 = scaler_y.inverse_transform(y_test_seq_v2)\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_mse_v2 = mean_squared_error(y_test_actual_v2, lstm_pred_v2)\n",
        "lstm_rmse_v2 = np.sqrt(lstm_mse_v2)\n",
        "lstm_mape_v2 = mean_absolute_percentage_error(y_test_actual_v2, lstm_pred_v2) * 100\n",
        "\n",
        "print(\"\\nImproved LSTM Model Metrics:\")\n",
        "print(f\"MSE: {lstm_mse_v2:.2f}\")\n",
        "print(f\"RMSE: {lstm_rmse_v2:.2f}\")\n",
        "print(f\"MAPE: {lstm_mape_v2:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yYI-KU84XDZU"
      },
      "outputs": [],
      "source": [
        "# 1. First, let's check where the NaN values are coming from\n",
        "print(\"NaN values in X_train:\", np.isnan(X_train).any())\n",
        "print(\"NaN values in y_train:\", np.isnan(y_train).any())\n",
        "\n",
        "# 2. Clean the data and handle NaN values\n",
        "def clean_and_prepare_data(X, y):\n",
        "    # Fill NaN values in features with mean\n",
        "    X_clean = pd.DataFrame(X).fillna(pd.DataFrame(X).mean())\n",
        "\n",
        "    # Remove any rows where target is NaN\n",
        "    valid_idx = ~pd.isna(y)\n",
        "    X_clean = X_clean[valid_idx]\n",
        "    y_clean = y[valid_idx]\n",
        "\n",
        "    return X_clean.values, y_clean\n",
        "\n",
        "X_train_clean, y_train_clean = clean_and_prepare_data(X_train, y_train)\n",
        "X_test_clean, y_test_clean = clean_and_prepare_data(X_test, y_test)\n",
        "\n",
        "# 3. Modified sequence preparation function\n",
        "def prepare_time_series_data_v3(X, y, sequence_length=5):\n",
        "    # Scale the data\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    # Create sequences\n",
        "    X_sequences = []\n",
        "    y_sequences = []\n",
        "\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
        "        y_sequences.append(y_scaled[i + sequence_length])\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "# 4. Prepare sequences with clean data\n",
        "X_train_seq, y_train_seq = prepare_time_series_data_v3(X_train_clean, y_train_clean)\n",
        "X_test_seq, y_test_seq = prepare_time_series_data_v3(X_test_clean, y_test_clean)\n",
        "\n",
        "# 5. Build and train the model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, X_train_clean.shape[1])),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train with early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Evaluate\n",
        "lstm_pred = lstm_model.predict(X_test_seq)\n",
        "lstm_pred = scaler_y.inverse_transform(lstm_pred)\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_mse = mean_squared_error(y_test_actual, lstm_pred)\n",
        "lstm_rmse = np.sqrt(lstm_mse)\n",
        "lstm_mape = mean_absolute_percentage_error(y_test_actual, lstm_pred) * 100\n",
        "\n",
        "print(\"\\nLSTM Model Metrics:\")\n",
        "print(f\"MSE: {lstm_mse:.2f}\")\n",
        "print(f\"RMSE: {lstm_rmse:.2f}\")\n",
        "print(f\"MAPE: {lstm_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4MtIcIayXSP7"
      },
      "outputs": [],
      "source": [
        "def prepare_time_series_data_v3(X, y, sequence_length=5):\n",
        "    # Convert inputs to numpy arrays if they're pandas Series/DataFrame\n",
        "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
        "        X = X.to_numpy()\n",
        "    if isinstance(y, pd.Series):\n",
        "        y = y.to_numpy()\n",
        "\n",
        "    # Reshape y if needed\n",
        "    if len(y.shape) == 1:\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "    # Scale the data\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "    # Create sequences\n",
        "    X_sequences = []\n",
        "    y_sequences = []\n",
        "\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_sequences.append(X_scaled[i:(i + sequence_length)])\n",
        "        y_sequences.append(y_scaled[i + sequence_length])\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n",
        "\n",
        "# Clean data function\n",
        "def clean_and_prepare_data(X, y):\n",
        "    # Convert to DataFrame if not already\n",
        "    X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
        "\n",
        "    # Fill NaN values in features with mean\n",
        "    X_clean = X_df.fillna(X_df.mean())\n",
        "\n",
        "    # Remove any rows where target is NaN\n",
        "    if isinstance(y, pd.Series):\n",
        "        valid_idx = ~y.isna()\n",
        "    else:\n",
        "        valid_idx = ~pd.isna(y)\n",
        "\n",
        "    X_clean = X_clean[valid_idx]\n",
        "    y_clean = y[valid_idx]\n",
        "\n",
        "    return X_clean, y_clean\n",
        "\n",
        "# Prepare the data\n",
        "X_train_clean, y_train_clean = clean_and_prepare_data(X_train, y_train)\n",
        "X_test_clean, y_test_clean = clean_and_prepare_data(X_test, y_test)\n",
        "\n",
        "# Create sequences\n",
        "X_train_seq, y_train_seq = prepare_time_series_data_v3(X_train_clean, y_train_clean)\n",
        "X_test_seq, y_test_seq = prepare_time_series_data_v3(X_test_clean, y_test_clean)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Training sequence shape:\", X_train_seq.shape)\n",
        "print(\"Training target shape:\", y_train_seq.shape)\n",
        "print(\"Test sequence shape:\", X_test_seq.shape)\n",
        "print(\"Test target shape:\", y_test_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lX8boK3bXir5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Build LSTM model with improved architecture\n",
        "lstm_model = Sequential([\n",
        "    # First LSTM layer with more units since we have 110 features\n",
        "    LSTM(128, input_shape=(sequence_length, 110), return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Second LSTM layer\n",
        "    LSTM(64, return_sequences=False),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Dense layers for final predictions\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile with appropriate loss and metrics\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae', 'mape']\n",
        ")\n",
        "\n",
        "# Add callbacks for better training\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = lstm_model.fit(\n",
        "    X_train_seq,\n",
        "    y_train_seq,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = lstm_model.predict(X_test_seq)\n",
        "\n",
        "# Inverse transform predictions and actual values\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "mse = mean_squared_error(y_test_actual, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_actual, y_pred)\n",
        "mape = mean_absolute_percentage_error(y_test_actual, y_pred) * 100\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Io3l9IbcYtVF"
      },
      "outputs": [],
      "source": [
        "# 1. Let's first normalize the target variable using log transformation\n",
        "def preprocess_data(X, y):\n",
        "    # Log transform the target (adding 1 to handle zeros)\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    # Scale features using robust scaler\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y_log.reshape(-1, 1))\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
        "\n",
        "# 2. Prepare sequences with better handling of time dependency\n",
        "def create_sequences_v2(X, y, sequence_length=5):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_seq.append(X[i:(i + sequence_length)])\n",
        "        y_seq.append(y[i + sequence_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# 3. Preprocess and prepare data\n",
        "X_train_scaled, y_train_scaled, scaler_X, scaler_y = preprocess_data(X_train_clean, y_train_clean)\n",
        "X_test_scaled, y_test_scaled, _, _ = preprocess_data(X_test_clean, y_test_clean)\n",
        "\n",
        "# 4. Create sequences\n",
        "X_train_seq, y_train_seq = create_sequences_v2(X_train_scaled, y_train_scaled)\n",
        "X_test_seq, y_test_seq = create_sequences_v2(X_test_scaled, y_test_scaled)\n",
        "\n",
        "# 5. Build a simpler model with regularization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, X_train_clean.shape[1]),\n",
        "         return_sequences=True, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 6. Compile with Huber loss for robustness\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# 7. Train with callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8. Evaluate and inverse transform predictions\n",
        "y_pred = model.predict(X_test_seq)\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "y_pred = np.expm1(y_pred)  # Inverse of log1p\n",
        "\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
        "y_test_actual = np.expm1(y_test_actual)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_actual, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = mean_absolute_percentage_error(y_test_actual, y_pred) * 100\n",
        "\n",
        "print(\"\\nImproved Model Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WTcKOXwyZJ58"
      },
      "outputs": [],
      "source": [
        "# 1. Let's first normalize the target variable using log transformation\n",
        "def preprocess_data(X, y):\n",
        "    # Log transform the target (adding 1 to handle zeros)\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    # Scale features using robust scaler\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "\n",
        "    X_scaled = scaler_X.fit_transform(X)\n",
        "    y_scaled = scaler_y.fit_transform(y_log.reshape(-1, 1))\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
        "\n",
        "# 2. Prepare sequences with better handling of time dependency\n",
        "def create_sequences_v2(X, y, sequence_length=5):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_seq.append(X[i:(i + sequence_length)])\n",
        "        y_seq.append(y[i + sequence_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# 3. Preprocess and prepare data\n",
        "X_train_scaled, y_train_scaled, scaler_X, scaler_y = preprocess_data(X_train_clean, y_train_clean)\n",
        "X_test_scaled, y_test_scaled, _, _ = preprocess_data(X_test_clean, y_test_clean)\n",
        "\n",
        "# 4. Create sequences\n",
        "X_train_seq, y_train_seq = create_sequences_v2(X_train_scaled, y_train_scaled)\n",
        "X_test_seq, y_test_seq = create_sequences_v2(X_test_scaled, y_test_scaled)\n",
        "\n",
        "# 5. Build a simpler model with regularization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, X_train_clean.shape[1]),\n",
        "         return_sequences=True, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32, kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 6. Compile with Huber loss for robustness\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# 7. Train with callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8. Evaluate and inverse transform predictions\n",
        "y_pred = model.predict(X_test_seq)\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "y_pred = np.expm1(y_pred)  # Inverse of log1p\n",
        "\n",
        "y_test_actual = scaler_y.inverse_transform(y_test_seq)\n",
        "y_test_actual = np.expm1(y_test_actual)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_actual, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mape = mean_absolute_percentage_error(y_test_actual, y_pred) * 100\n",
        "\n",
        "print(\"\\nImproved Model Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ZZ5y-caZPyN"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(X, y):\n",
        "    # Convert to numpy arrays first\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X = X.values\n",
        "    if isinstance(y, pd.Series):\n",
        "        y = y.values\n",
        "\n",
        "    # Handle zeros and negatives before log transform\n",
        "    y_min = np.min(y)\n",
        "    if y_min <= 0:\n",
        "        y = y - y_min + 1  # Shift data to make it positive\n",
        "\n",
        "    # Log transform the target\n",
        "    y_log = np.log1p(y)\n",
        "\n",
        "    # Scale features and target\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "\n",
        "    # Ensure correct shapes\n",
        "    X_reshaped = X.reshape((X.shape[0], -1))  # Flatten if needed\n",
        "    y_reshaped = y_log.reshape(-1, 1)\n",
        "\n",
        "    X_scaled = scaler_X.fit_transform(X_reshaped)\n",
        "    y_scaled = scaler_y.fit_transform(y_reshaped)\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y, y_min\n",
        "\n",
        "# Let's try a simpler sequence creation approach\n",
        "def create_sequences_v2(X, y, sequence_length=5):\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_seq.append(X[i:(i + sequence_length)])\n",
        "        y_seq.append(y[i + sequence_length])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq).reshape(-1, 1)\n",
        "\n",
        "# Preprocess data\n",
        "X_train_scaled, y_train_scaled, scaler_X, scaler_y, y_min = preprocess_data(X_train_clean, y_train_clean)\n",
        "X_test_scaled, y_test_scaled, _, _, _ = preprocess_data(X_test_clean, y_test_clean)\n",
        "\n",
        "# Create sequences\n",
        "X_train_seq, y_train_seq = create_sequences_v2(X_train_scaled, y_train_scaled)\n",
        "X_test_seq, y_test_seq = create_sequences_v2(X_test_scaled, y_test_scaled)\n",
        "\n",
        "print(\"Sequence shapes:\")\n",
        "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
        "print(\"y_train_seq shape:\", y_train_seq.shape)\n",
        "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
        "print(\"y_test_seq shape:\", y_test_seq.shape)\n",
        "\n",
        "# If shapes look correct, we can proceed with model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7MrbK6XoZafI"
      },
      "outputs": [],
      "source": [
        "# Build the final model with an optimized architecture\n",
        "model = Sequential([\n",
        "    # First LSTM layer with careful tuning\n",
        "    LSTM(128, input_shape=(sequence_length, 110),\n",
        "         return_sequences=True,\n",
        "         kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Second LSTM layer\n",
        "    LSTM(64),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Dense layers with careful size reduction\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile with Huber loss for robustness\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Add callbacks for training optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_seq,\n",
        "    y_train_seq,\n",
        "    validation_split=0.2,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Make predictions and inverse transform\n",
        "y_pred = model.predict(X_test_seq)\n",
        "y_pred_transformed = scaler_y.inverse_transform(y_pred)\n",
        "y_test_transformed = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Inverse the log transformation and shift\n",
        "y_pred_final = np.expm1(y_pred_transformed)\n",
        "y_test_final = np.expm1(y_test_transformed)\n",
        "\n",
        "if y_min <= 0:\n",
        "    y_pred_final = y_pred_final + y_min - 1\n",
        "    y_test_final = y_test_final + y_min - 1\n",
        "\n",
        "# Calculate final metrics\n",
        "mse = mean_squared_error(y_test_final, y_pred_final)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_final, y_pred_final)\n",
        "mape = mean_absolute_percentage_error(y_test_final, y_pred_final) * 100\n",
        "\n",
        "print(\"\\nFinal Model Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NrL0zi94Zkjm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Build the final model\n",
        "model = Sequential([\n",
        "    # First LSTM layer\n",
        "    LSTM(128,\n",
        "         input_shape=(sequence_length, 110),\n",
        "         return_sequences=True,\n",
        "         kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Second LSTM layer\n",
        "    LSTM(64,\n",
        "         kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Dense layers\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile with Huber loss\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Training callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_seq,\n",
        "    y_train_seq,\n",
        "    validation_split=0.2,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Print model summary to verify architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lBOJghVZePr7"
      },
      "outputs": [],
      "source": [
        "# Make predictions and inverse transform\n",
        "y_pred = model.predict(X_test_seq)\n",
        "y_pred_transformed = scaler_y.inverse_transform(y_pred)\n",
        "y_test_transformed = scaler_y.inverse_transform(y_test_seq)\n",
        "\n",
        "# Inverse the log transformation\n",
        "y_pred_final = np.expm1(y_pred_transformed)\n",
        "y_test_final = np.expm1(y_test_transformed)\n",
        "\n",
        "if y_min <= 0:\n",
        "    y_pred_final = y_pred_final + y_min - 1\n",
        "    y_test_final = y_test_final + y_min - 1\n",
        "\n",
        "# Calculate final metrics\n",
        "mse = mean_squared_error(y_test_final, y_pred_final)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_final, y_pred_final)\n",
        "mape = mean_absolute_percentage_error(y_test_final, y_pred_final) * 100\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nFinal Model Metrics:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_final[:100], label='Actual', marker='o', alpha=0.5)\n",
        "plt.plot(y_pred_final[:100], label='Predicted', marker='x', alpha=0.5)\n",
        "plt.title('Energy Consumption: Actual vs Predicted (First 100 samples)')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate prediction intervals (95% confidence)\n",
        "from scipy import stats\n",
        "\n",
        "residuals = y_test_final - y_pred_final\n",
        "std_dev = np.std(residuals)\n",
        "confidence_interval = 1.96 * std_dev  # 95% confidence interval\n",
        "\n",
        "print(\"\\nPrediction Interval (95% confidence):\")\n",
        "print(f\"{confidence_interval:.2f} units\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GW5Vw4u3ejsG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_final[:100], label='Actual', marker='o', alpha=0.5)\n",
        "plt.plot(y_pred_final[:100], label='Predicted', marker='x', alpha=0.5)\n",
        "plt.title('Energy Consumption: Actual vs Predicted (First 100 samples)')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Now let's create future predictions (2021-2030)\n",
        "def create_future_features(last_sequence, num_predictions, sequence_length=5):\n",
        "    future_sequences = []\n",
        "    current_sequence = last_sequence.copy()\n",
        "\n",
        "    for _ in range(num_predictions):\n",
        "        # Predict next value\n",
        "        sequence_reshaped = current_sequence.reshape((1, sequence_length, -1))\n",
        "        next_pred = model.predict(sequence_reshaped)[0]\n",
        "\n",
        "        # Update sequence\n",
        "        current_sequence = np.roll(current_sequence, -1, axis=0)\n",
        "        current_sequence[-1] = next_pred\n",
        "\n",
        "        future_sequences.append(next_pred)\n",
        "\n",
        "    return np.array(future_sequences)\n",
        "\n",
        "# Get last sequence from training data\n",
        "last_sequence = X_test_seq[-1]\n",
        "\n",
        "# Generate predictions for next 10 years (2021-2030)\n",
        "future_years = 10\n",
        "future_predictions = create_future_features(last_sequence, future_years)\n",
        "\n",
        "# Inverse transform predictions\n",
        "future_pred_transformed = scaler_y.inverse_transform(future_predictions)\n",
        "future_pred_final = np.expm1(future_pred_transformed)\n",
        "if y_min <= 0:\n",
        "    future_pred_final = future_pred_final + y_min - 1\n",
        "\n",
        "# Create years for x-axis\n",
        "years = np.arange(2021, 2031)\n",
        "\n",
        "# Plot future predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(years, future_pred_final, marker='o', linestyle='-', label='Forecasted Values')\n",
        "plt.fill_between(years,\n",
        "                future_pred_final - confidence_interval,\n",
        "                future_pred_final + confidence_interval,\n",
        "                alpha=0.2, label='95% Confidence Interval')\n",
        "plt.title('Energy Consumption Forecast (2021-2030)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print forecasted values\n",
        "print(\"\\nForecasted Energy Consumption (2021-2030):\")\n",
        "for year, pred in zip(years, future_pred_final):\n",
        "    print(f\"Year {year}: {pred[0]:.2f} {confidence_interval:.2f} units\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XI5RnyKAfs1v"
      },
      "outputs": [],
      "source": [
        "# First, let's organize our historical data by year\n",
        "def prepare_historical_data(df_melted):\n",
        "    # Annual total energy consumption\n",
        "    annual_total = df_melted.groupby('Year')['Consumption'].sum().reset_index()\n",
        "\n",
        "    # Energy carrier wise consumption\n",
        "    carrier_consumption = df_melted.groupby(['Year', 'Energy Carriers'])['Consumption'].sum().reset_index()\n",
        "\n",
        "    # Sector wise consumption (using Homogeneous Branches)\n",
        "    sector_consumption = df_melted.groupby(['Year', 'Homogeneous Branches'])['Consumption'].sum().reset_index()\n",
        "\n",
        "    # Get top 6 sectors by total consumption\n",
        "    top_sectors = sector_consumption.groupby('Homogeneous Branches')['Consumption'].sum()\\\n",
        "                                  .sort_values(ascending=False).head(6).index\n",
        "\n",
        "    return annual_total, carrier_consumption, sector_consumption, top_sectors\n",
        "\n",
        "# Prepare historical data\n",
        "annual_total, carrier_consumption, sector_consumption, top_sectors = prepare_historical_data(df_melted)\n",
        "\n",
        "# Create comprehensive visualizations\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# 1. Annual Total Energy Consumption (1995-2030)\n",
        "plt.figure(figsize=(15, 8))\n",
        "# Historical data\n",
        "plt.plot(annual_total['Year'], annual_total['Consumption'],\n",
        "         marker='o', label='Historical', linewidth=2)\n",
        "# Forecasted data\n",
        "plt.plot(years, future_pred_final,\n",
        "         marker='s', linestyle='--', label='Forecasted', linewidth=2)\n",
        "plt.fill_between(years,\n",
        "                future_pred_final.flatten() - confidence_interval,\n",
        "                future_pred_final.flatten() + confidence_interval,\n",
        "                alpha=0.2, label='95% Confidence Interval')\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Energy Carrier-wise Consumption\n",
        "# Get top 5 energy carriers\n",
        "top_carriers = carrier_consumption.groupby('Energy Carriers')['Consumption'].sum()\\\n",
        "                                .sort_values(ascending=False).head(5).index\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for carrier in top_carriers:\n",
        "    carrier_data = carrier_consumption[carrier_consumption['Energy Carriers'] == carrier]\n",
        "    plt.plot(carrier_data['Year'], carrier_data['Consumption'],\n",
        "            marker='o', label=carrier, linewidth=2)\n",
        "plt.title('Top 5 Energy Carriers Consumption Trends (1995-2020)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Top 6 Sectors Energy Consumption\n",
        "plt.figure(figsize=(15, 8))\n",
        "for sector in top_sectors:\n",
        "    sector_data = sector_consumption[sector_consumption['Homogeneous Branches'] == sector]\n",
        "    plt.plot(sector_data['Year'], sector_data['Consumption'],\n",
        "            marker='o', label=sector[:30] + '...', linewidth=2)\n",
        "plt.title('Top 6 Sectors Energy Consumption Trends (1995-2020)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary of Forecasted Total Energy Consumption:\")\n",
        "print(\"Average annual growth rate (2021-2030):\",\n",
        "      f\"{((future_pred_final[-1]/future_pred_final[0])**(1/10) - 1)*100:.2f}%\")\n",
        "print(\"\\nTop Energy Carriers (by total consumption):\")\n",
        "for i, carrier in enumerate(top_carriers, 1):\n",
        "    print(f\"{i}. {carrier}\")\n",
        "print(\"\\nTop Sectors (by total consumption):\")\n",
        "for i, sector in enumerate(top_sectors, 1):\n",
        "    print(f\"{i}. {sector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C7FohO45f8AS"
      },
      "outputs": [],
      "source": [
        "# First install seaborn if not already installed\n",
        "!pip install seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Annual Total Energy Consumption (1995-2030)\n",
        "plt.figure(figsize=(15, 8))\n",
        "# Historical data\n",
        "plt.plot(annual_total['Year'], annual_total['Consumption'],\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "# Forecasted data\n",
        "plt.plot(years, future_pred_final,\n",
        "         marker='s', linestyle='--', label='Forecasted', linewidth=2, color='red')\n",
        "plt.fill_between(years,\n",
        "                future_pred_final.flatten() - confidence_interval,\n",
        "                future_pred_final.flatten() + confidence_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 2. Energy Carrier-wise Consumption\n",
        "# Get top 5 energy carriers\n",
        "top_carriers = carrier_consumption.groupby('Energy Carriers')['Consumption'].sum()\\\n",
        "                                .sort_values(ascending=False).head(5).index\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
        "for carrier, color in zip(top_carriers, colors):\n",
        "    carrier_data = carrier_consumption[carrier_consumption['Energy Carriers'] == carrier]\n",
        "    plt.plot(carrier_data['Year'], carrier_data['Consumption'],\n",
        "            marker='o', label=carrier, linewidth=2, color=color)\n",
        "plt.title('Top 5 Energy Carriers Consumption Trends (1995-2020)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Top 6 Sectors Energy Consumption\n",
        "plt.figure(figsize=(15, 8))\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown']\n",
        "for sector, color in zip(top_sectors, colors):\n",
        "    sector_data = sector_consumption[sector_consumption['Homogeneous Branches'] == sector]\n",
        "    plt.plot(sector_data['Year'], sector_data['Consumption'],\n",
        "            marker='o', label=sector[:30] + '...', linewidth=2, color=color)\n",
        "plt.title('Top 6 Sectors Energy Consumption Trends (1995-2020)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nSummary of Forecasted Total Energy Consumption:\")\n",
        "print(\"Average annual growth rate (2021-2030):\",\n",
        "      f\"{((future_pred_final[-1]/future_pred_final[0])**(1/10) - 1)*100:.2f}%\")\n",
        "print(\"\\nTop Energy Carriers (by total consumption):\")\n",
        "for i, carrier in enumerate(top_carriers, 1):\n",
        "    print(f\"{i}. {carrier}\")\n",
        "print(\"\\nTop Sectors (by total consumption):\")\n",
        "for i, sector in enumerate(top_sectors, 1):\n",
        "    print(f\"{i}. {sector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k1cguJkmgeM_"
      },
      "outputs": [],
      "source": [
        "# Calculate residuals and confidence interval properly\n",
        "def calculate_confidence_interval(y_true, y_pred, confidence=0.95):\n",
        "    residuals = y_true - y_pred\n",
        "    std_dev = np.std(residuals)\n",
        "    z_score = stats.norm.ppf((1 + confidence) / 2)\n",
        "    return z_score * std_dev\n",
        "\n",
        "# 1. Annual Total Energy Consumption (1995-2030)\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Historical data\n",
        "plt.plot(annual_total['Year'], annual_total['Consumption'],\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "\n",
        "# Calculate confidence interval\n",
        "conf_interval = calculate_confidence_interval(y_test_actual, y_pred)\n",
        "\n",
        "# Forecasted data\n",
        "future_years = np.arange(2021, 2031)\n",
        "plt.plot(future_years, future_pred_final,\n",
        "         marker='s', linestyle='--', label='Forecasted', linewidth=2, color='red')\n",
        "\n",
        "# Add confidence intervals\n",
        "plt.fill_between(future_years,\n",
        "                future_pred_final.flatten() - conf_interval,\n",
        "                future_pred_final.flatten() + conf_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 2. Energy Carrier-wise Consumption with forecasts\n",
        "plt.figure(figsize=(15, 8))\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
        "for carrier, color in zip(top_carriers, colors):\n",
        "    carrier_data = carrier_consumption[carrier_consumption['Energy Carriers'] == carrier]\n",
        "\n",
        "    # Historical data\n",
        "    plt.plot(carrier_data['Year'], carrier_data['Consumption'],\n",
        "            marker='o', label=f'{carrier} (Historical)', linewidth=2, color=color)\n",
        "\n",
        "    # Add forecast line (simple trend extrapolation)\n",
        "    x = carrier_data['Year'].values.reshape(-1, 1)\n",
        "    y = carrier_data['Consumption'].values\n",
        "    model = LinearRegression()\n",
        "    model.fit(x, y)\n",
        "\n",
        "    # Forecast for 2021-2030\n",
        "    future_x = np.arange(2021, 2031).reshape(-1, 1)\n",
        "    future_y = model.predict(future_x)\n",
        "    plt.plot(future_x, future_y, linestyle='--', color=color,\n",
        "            label=f'{carrier} (Forecast)')\n",
        "\n",
        "plt.title('Energy Carriers Consumption and Forecasts (1995-2030)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Top 6 Sectors Energy Consumption with forecasts\n",
        "plt.figure(figsize=(15, 8))\n",
        "colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown']\n",
        "for sector, color in zip(top_sectors, colors):\n",
        "    sector_data = sector_consumption[sector_consumption['Homogeneous Branches'] == sector]\n",
        "\n",
        "    # Historical data\n",
        "    plt.plot(sector_data['Year'], sector_data['Consumption'],\n",
        "            marker='o', label=f'{sector[:30]}... (Historical)', linewidth=2, color=color)\n",
        "\n",
        "    # Add forecast line\n",
        "    x = sector_data['Year'].values.reshape(-1, 1)\n",
        "    y = sector_data['Consumption'].values\n",
        "    model = LinearRegression()\n",
        "    model.fit(x, y)\n",
        "\n",
        "    # Forecast for 2021-2030\n",
        "    future_x = np.arange(2021, 2031).reshape(-1, 1)\n",
        "    future_y = model.predict(future_x)\n",
        "    plt.plot(future_x, future_y, linestyle='--', color=color,\n",
        "            label=f'{sector[:30]}... (Forecast)')\n",
        "\n",
        "plt.title('Top 6 Sectors Energy Consumption and Forecasts (1995-2030)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print trend analysis\n",
        "print(\"\\nTrend Analysis (2021-2030):\")\n",
        "print(\"\\nEnergy Carriers Trends:\")\n",
        "for carrier in top_carriers:\n",
        "    carrier_data = carrier_consumption[carrier_consumption['Energy Carriers'] == carrier]\n",
        "    trend = np.polyfit(carrier_data['Year'], carrier_data['Consumption'], 1)[0]\n",
        "    print(f\"{carrier}: {'Increasing' if trend > 0 else 'Decreasing'} trend\")\n",
        "\n",
        "print(\"\\nSector Trends:\")\n",
        "for sector in top_sectors:\n",
        "    sector_data = sector_consumption[sector_consumption['Homogeneous Branches'] == sector]\n",
        "    trend = np.polyfit(sector_data['Year'], sector_data['Consumption'], 1)[0]\n",
        "    print(f\"{sector[:50]}...: {'Increasing' if trend > 0 else 'Decreasing'} trend\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2z5X8XBlgw1-"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Recalculate confidence interval\n",
        "def calculate_confidence_interval(y_true, y_pred, confidence=0.95):\n",
        "    residuals = y_true - y_pred\n",
        "    std_dev = np.std(residuals)\n",
        "    z_score = stats.norm.ppf((1 + confidence) / 2)\n",
        "    return z_score * std_dev\n",
        "\n",
        "# 1. Annual Total Energy Consumption (1995-2030)\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Historical data\n",
        "x_historical = annual_total['Year'].values\n",
        "y_historical = annual_total['Consumption'].values\n",
        "plt.plot(x_historical, y_historical,\n",
        "         marker='o', label='Historical (1995-2020)', linewidth=2, color='blue')\n",
        "\n",
        "# Future years for prediction\n",
        "x_future = np.arange(2021, 2031)\n",
        "\n",
        "# Predicted data (reshape for plotting)\n",
        "y_predicted = future_pred_final.reshape(-1)\n",
        "plt.plot(x_future, y_predicted,\n",
        "         marker='s', linestyle='--', label='Predicted (2021-2030)',\n",
        "         linewidth=2, color='red')\n",
        "\n",
        "# Add confidence intervals\n",
        "conf_interval = calculate_confidence_interval(y_test_actual, y_pred)\n",
        "plt.fill_between(x_future,\n",
        "                y_predicted - conf_interval,\n",
        "                y_predicted + conf_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14, pad=20)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(fontsize=10)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add annotations for key points\n",
        "plt.text(2020, y_historical[-1], f'Last Historical: {y_historical[-1]:.0f}',\n",
        "         verticalalignment='bottom')\n",
        "plt.text(2029, y_predicted[-1], f'Final Prediction: {y_predicted[-1]:.0f}',\n",
        "         verticalalignment='bottom')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print prediction summary\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Last Historical Value (2020): {y_historical[-1]:.2f}\")\n",
        "print(f\"First Prediction (2021): {y_predicted[0]:.2f}\")\n",
        "print(f\"Final Prediction (2030): {y_predicted[-1]:.2f}\")\n",
        "print(f\"95% Confidence Interval: {conf_interval:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MPnLFfaohNOF"
      },
      "outputs": [],
      "source": [
        "# 1. Improved preprocessing with feature engineering\n",
        "def create_features(df_melted):\n",
        "    # Create yearly aggregates\n",
        "    yearly_data = df_melted.groupby('Year')['Consumption'].agg([\n",
        "        ('total', 'sum'),\n",
        "        ('mean', 'mean'),\n",
        "        ('std', 'std'),\n",
        "        ('min', 'min'),\n",
        "        ('max', 'max')\n",
        "    ]).reset_index()\n",
        "\n",
        "    # Add trend features\n",
        "    yearly_data['year_norm'] = (yearly_data['Year'] - yearly_data['Year'].min()) / \\\n",
        "                              (yearly_data['Year'].max() - yearly_data['Year'].min())\n",
        "    yearly_data['trend'] = np.arange(len(yearly_data))\n",
        "\n",
        "    # Add cyclical features\n",
        "    yearly_data['year_sin'] = np.sin(2 * np.pi * yearly_data['year_norm'])\n",
        "    yearly_data['year_cos'] = np.cos(2 * np.pi * yearly_data['year_norm'])\n",
        "\n",
        "    return yearly_data\n",
        "\n",
        "# 2. Improved model architecture\n",
        "def build_improved_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(128, input_shape=input_shape, return_sequences=True,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        LSTM(64, return_sequences=True,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        LSTM(32, kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='huber',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# 3. Prepare data\n",
        "yearly_features = create_features(df_melted)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences_with_features(data, sequence_length=5):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data.iloc[i:(i + sequence_length)].values)\n",
        "        y.append(data.iloc[i + sequence_length]['total'])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Scale features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X = yearly_features.drop(['Year', 'total'], axis=1)\n",
        "y = yearly_features['total']\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 5\n",
        "X_seq, y_seq = create_sequences_with_features(yearly_features, sequence_length)\n",
        "\n",
        "# Split data\n",
        "split_idx = int(len(X_seq) * 0.8)\n",
        "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "# 4. Train improved model\n",
        "improved_model = build_improved_model((sequence_length, X.shape[1]))\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "history = improved_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Make predictions\n",
        "def make_future_predictions(model, last_sequence, n_steps=10):\n",
        "    predictions = []\n",
        "    curr_seq = last_sequence.copy()\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        pred = model.predict(curr_seq.reshape(1, sequence_length, -1))[0]\n",
        "        predictions.append(pred)\n",
        "\n",
        "        # Update sequence\n",
        "        new_row = curr_seq[-1].copy()\n",
        "        new_row[0] = pred  # Update the total value\n",
        "        curr_seq = np.vstack([curr_seq[1:], new_row])\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Get predictions\n",
        "last_sequence = X_test[-1:]\n",
        "future_predictions = make_future_predictions(improved_model, last_sequence)\n",
        "future_predictions = scaler_y.inverse_transform(future_predictions)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(yearly_features['Year'], yearly_features['total'],\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "\n",
        "future_years = np.arange(2021, 2031)\n",
        "plt.plot(future_years, future_predictions,\n",
        "         marker='s', linestyle='--', label='Predicted', linewidth=2, color='red')\n",
        "\n",
        "conf_interval = calculate_confidence_interval(y_test,\n",
        "                                           improved_model.predict(X_test).flatten())\n",
        "plt.fill_between(future_years,\n",
        "                future_predictions.flatten() - conf_interval,\n",
        "                future_predictions.flatten() + conf_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print prediction summary\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Last Historical Value (2020): {yearly_features['total'].iloc[-1]:,.2f}\")\n",
        "print(f\"First Prediction (2021): {future_predictions[0][0]:,.2f}\")\n",
        "print(f\"Final Prediction (2030): {future_predictions[-1][0]:,.2f}\")\n",
        "print(f\"Predicted Change 2020-2030: {((future_predictions[-1][0]/yearly_features['total'].iloc[-1])-1)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S5k6IQJChdtA"
      },
      "outputs": [],
      "source": [
        "# 1. First, let's check our data dimensions\n",
        "def print_data_info(X, y, title=\"Dataset Info\"):\n",
        "    print(f\"\\n{title}\")\n",
        "    print(f\"X shape: {X.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    print(f\"Number of features: {X.shape[-1]}\")\n",
        "\n",
        "# 2. Create sequences with correct dimensions\n",
        "def create_sequences_with_features(data, sequence_length=5):\n",
        "    features = data.drop(['Year', 'total'], axis=1).values\n",
        "    target = data['total'].values\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(features[i:(i + sequence_length)])\n",
        "        y.append(target[i + sequence_length])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 3. Build model with correct input shape\n",
        "def build_improved_model(input_shape):\n",
        "    model = Sequential([\n",
        "        # First LSTM layer\n",
        "        LSTM(64, input_shape=input_shape, return_sequences=True,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Second LSTM layer\n",
        "        LSTM(32, return_sequences=False,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Dense layers\n",
        "        Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='huber',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# 4. Prepare data\n",
        "sequence_length = 5\n",
        "yearly_features = create_features(df_melted)\n",
        "\n",
        "# Create and scale sequences\n",
        "X_seq, y_seq = create_sequences_with_features(yearly_features, sequence_length)\n",
        "\n",
        "# Scale features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Reshape for scaling\n",
        "X_seq_reshaped = X_seq.reshape(-1, X_seq.shape[-1])\n",
        "X_seq_scaled = scaler_X.fit_transform(X_seq_reshaped)\n",
        "X_seq_scaled = X_seq_scaled.reshape(X_seq.shape)\n",
        "y_seq_scaled = scaler_y.fit_transform(y_seq.reshape(-1, 1))\n",
        "\n",
        "# Split data\n",
        "split_idx = int(len(X_seq_scaled) * 0.8)\n",
        "X_train = X_seq_scaled[:split_idx]\n",
        "X_test = X_seq_scaled[split_idx:]\n",
        "y_train = y_seq_scaled[:split_idx]\n",
        "y_test = y_seq_scaled[split_idx:]\n",
        "\n",
        "# Print shapes to verify\n",
        "print_data_info(X_train, y_train, \"Training Data\")\n",
        "print_data_info(X_test, y_test, \"Test Data\")\n",
        "\n",
        "# 5. Build and train model\n",
        "input_shape = (sequence_length, X_train.shape[2])\n",
        "improved_model = build_improved_model(input_shape)\n",
        "\n",
        "# Print model summary\n",
        "improved_model.summary()\n",
        "\n",
        "# Train model\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "history = improved_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Make predictions\n",
        "def make_future_predictions(model, last_sequence, scaler_y, n_steps=10):\n",
        "    predictions = []\n",
        "    curr_seq = last_sequence.copy()\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        pred = model.predict(curr_seq.reshape(1, *curr_seq.shape), verbose=0)[0]\n",
        "        predictions.append(pred)\n",
        "\n",
        "        # Update sequence\n",
        "        curr_seq = np.roll(curr_seq, -1, axis=0)\n",
        "        curr_seq[-1] = pred\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    return scaler_y.inverse_transform(predictions)\n",
        "\n",
        "# Get predictions\n",
        "future_predictions = make_future_predictions(improved_model, X_test[-1:], scaler_y)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(yearly_features['Year'], yearly_features['total'],\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "\n",
        "future_years = np.arange(2021, 2031)\n",
        "plt.plot(future_years, future_predictions,\n",
        "         marker='s', linestyle='--', label='Predicted', linewidth=2, color='red')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Last Historical Value (2020): {yearly_features['total'].iloc[-1]:,.2f}\")\n",
        "print(f\"First Prediction (2021): {future_predictions[0][0]:,.2f}\")\n",
        "print(f\"Final Prediction (2030): {future_predictions[-1][0]:,.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jshOCxzfhwBG"
      },
      "outputs": [],
      "source": [
        "# Modified prediction function to handle shapes correctly\n",
        "def make_future_predictions(model, last_sequence, scaler_y, n_steps=10):\n",
        "    predictions = []\n",
        "    curr_seq = last_sequence.copy()\n",
        "\n",
        "    # Remove extra dimensions and ensure correct shape\n",
        "    curr_seq = curr_seq.reshape(-1, last_sequence.shape[-2], last_sequence.shape[-1])\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        # Make prediction\n",
        "        pred = model.predict(curr_seq, verbose=0)\n",
        "        predictions.append(pred[0])\n",
        "\n",
        "        # Update sequence (shift and add new prediction)\n",
        "        curr_seq = curr_seq.reshape(-1, last_sequence.shape[-2], last_sequence.shape[-1])\n",
        "        curr_seq = np.roll(curr_seq, -1, axis=1)\n",
        "        curr_seq[0, -1] = scaler_X.transform(pred.reshape(1, -1))\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    return scaler_y.inverse_transform(predictions.reshape(-1, 1))\n",
        "\n",
        "# Get predictions with corrected function\n",
        "last_sequence = X_test[-1:]\n",
        "future_predictions = make_future_predictions(improved_model, last_sequence, scaler_y)\n",
        "\n",
        "# Plot results with confidence intervals\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Historical data\n",
        "plt.plot(yearly_features['Year'], yearly_features['total'],\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "\n",
        "# Future predictions\n",
        "future_years = np.arange(2021, 2031)\n",
        "plt.plot(future_years, future_predictions,\n",
        "         marker='s', linestyle='--', label='Predicted', linewidth=2, color='red')\n",
        "\n",
        "# Calculate confidence intervals\n",
        "y_pred_test = improved_model.predict(X_test)\n",
        "residuals = y_test - y_pred_test\n",
        "confidence_interval = 1.96 * np.std(residuals)\n",
        "\n",
        "# Add confidence intervals\n",
        "plt.fill_between(future_years,\n",
        "                future_predictions.flatten() - confidence_interval,\n",
        "                future_predictions.flatten() + confidence_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Add annotations for important points\n",
        "plt.annotate(f'Last Historical (2020): {yearly_features[\"total\"].iloc[-1]:,.0f}',\n",
        "             xy=(2020, yearly_features['total'].iloc[-1]),\n",
        "             xytext=(10, 10), textcoords='offset points')\n",
        "plt.annotate(f'2030 Prediction: {future_predictions[-1][0]:,.0f}',\n",
        "             xy=(2030, future_predictions[-1][0]),\n",
        "             xytext=(10, -10), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed prediction summary\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Last Historical Value (2020): {yearly_features['total'].iloc[-1]:,.2f}\")\n",
        "print(f\"First Prediction (2021): {future_predictions[0][0]:,.2f}\")\n",
        "print(f\"Final Prediction (2030): {future_predictions[-1][0]:,.2f}\")\n",
        "print(f\"\\nPredicted Change (2020-2030): {((future_predictions[-1][0]/yearly_features['total'].iloc[-1])-1)*100:.1f}%\")\n",
        "print(f\"95% Confidence Interval: {confidence_interval:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WouZbL3SiAnm"
      },
      "outputs": [],
      "source": [
        "def make_future_predictions(model, last_sequence, scaler_y, n_steps=10):\n",
        "    predictions = []\n",
        "    curr_seq = last_sequence.copy()\n",
        "\n",
        "    # Keep track of the last prediction for feature updates\n",
        "    last_features = curr_seq[0, -1].copy()\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        # Make prediction with current sequence\n",
        "        pred = model.predict(curr_seq, verbose=0)\n",
        "        predictions.append(pred[0])\n",
        "\n",
        "        # Update features for next prediction\n",
        "        # Preserving the feature structure of last_features\n",
        "        next_features = last_features.copy()\n",
        "        next_features = next_features.reshape(1, -1)\n",
        "\n",
        "        # Roll the sequence and update last position\n",
        "        curr_seq = np.roll(curr_seq, -1, axis=1)\n",
        "        curr_seq[0, -1] = next_features\n",
        "\n",
        "        # Update last_features for next iteration\n",
        "        last_features = next_features[0]\n",
        "\n",
        "    # Convert predictions to numpy array and inverse transform\n",
        "    predictions = np.array(predictions)\n",
        "    return scaler_y.inverse_transform(predictions.reshape(-1, 1))\n",
        "\n",
        "# Let's verify our data shapes before making predictions\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Last sequence shape:\", X_test[-1:].shape)\n",
        "print(\"Number of features:\", X_test.shape[-1])\n",
        "\n",
        "# Get predictions\n",
        "last_sequence = X_test[-1:]\n",
        "future_predictions = make_future_predictions(improved_model, last_sequence, scaler_y)\n",
        "\n",
        "# Plot results with confidence intervals\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Historical data\n",
        "historical_years = yearly_features['Year']\n",
        "historical_values = yearly_features['total']\n",
        "plt.plot(historical_years, historical_values,\n",
        "         marker='o', label='Historical', linewidth=2, color='blue')\n",
        "\n",
        "# Future predictions\n",
        "future_years = np.arange(2021, 2031)\n",
        "plt.plot(future_years, future_predictions,\n",
        "         marker='s', linestyle='--', label='Predicted', linewidth=2, color='red')\n",
        "\n",
        "# Calculate and plot confidence intervals\n",
        "test_predictions = model.predict(X_test)\n",
        "test_predictions = scaler_y.inverse_transform(test_predictions)\n",
        "test_actual = scaler_y.inverse_transform(y_test)\n",
        "std_error = np.std(test_actual - test_predictions)\n",
        "confidence_interval = 1.96 * std_error\n",
        "\n",
        "plt.fill_between(future_years,\n",
        "                future_predictions.flatten() - confidence_interval,\n",
        "                future_predictions.flatten() + confidence_interval,\n",
        "                alpha=0.2, color='red', label='95% Confidence Interval')\n",
        "\n",
        "plt.title('Germany\\'s Total Energy Consumption (1995-2030)', fontsize=14)\n",
        "plt.xlabel('Year', fontsize=12)\n",
        "plt.ylabel('Energy Consumption (TJ)', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# Add annotations\n",
        "plt.annotate(f'Last Historical (2020): {historical_values.iloc[-1]:,.0f}',\n",
        "             xy=(2020, historical_values.iloc[-1]),\n",
        "             xytext=(10, 10), textcoords='offset points')\n",
        "plt.annotate(f'2030 Prediction: {future_predictions[-1][0]:,.0f}',\n",
        "             xy=(2030, future_predictions[-1][0]),\n",
        "             xytext=(10, -10), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nPrediction Summary:\")\n",
        "print(f\"Last Historical Value (2020): {historical_values.iloc[-1]:,.2f}\")\n",
        "print(f\"First Prediction (2021): {future_predictions[0][0]:,.2f}\")\n",
        "print(f\"Final Prediction (2030): {future_predictions[-1][0]:,.2f}\")\n",
        "print(f\"\\nPredicted Change (2020-2030): {((future_predictions[-1][0]/historical_values.iloc[-1])-1)*100:.1f}%\")\n",
        "print(f\"95% Confidence Interval: {confidence_interval:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcZxX0Mb+o1OSF6u01mMRg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}